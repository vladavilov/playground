# Technical Requirements Document: PDF Structured Product Data Extraction System

**Version:** 0.3
**Date:** 2024-07-28

## 1. Introduction

This document outlines the technical requirements for a system designed to parse a single PDF document containing information about a structured financial product. The system will extract a predefined, extensive list of data points (over 300 properties), organized into logical groups, and save the aggregated data into a structured Excel file. The system will leverage a Python-based workflow, incorporating an AI agent with a Retrieval Augmented Generation (RAG) approach for the core data extraction task.

## 2. System Overview

The system will operate through a sequential pipeline for each PDF processed:

1.  **PDF Ingestion & Text Preprocessing/Chunking:** The input PDF file is processed to extract clean text, which is then chunked for efficient RAG processing.
2.  **AI-Powered Data Extraction (Iterative & Grouped with RAG):** An AI agent iterates through predefined logical groups of properties. For each group, it uses a RAG approach to find relevant text sections and then prompts an LLM to extract the specific properties of that group into a JSON structure. Includes validation and refinement loops.
3.  **Data Aggregation & Output:** The extracted JSON data from all groups is aggregated and then formatted and saved into an Excel spreadsheet.

The PDF preprocessing/chunking and Excel output components will be implemented as Python scripts. The data extraction component will constitute the AI agent workflow.

## 3. Core Components & Technical Specifications

### 3.1. PDF Processing & Text Preprocessing/Chunking (Script)

*   **Description:** This component is responsible for ingesting a PDF file, converting it into clean, usable text, and then splitting it into manageable chunks suitable for the RAG process.
*   **Input:** A single PDF file (all PDFs have selectable text).
*   **Output:** A list of cleaned text chunks from the PDF.
*   **Technical Implementation:**
    *   **Language:** Python
    *   **Libraries (Recommended):**
        *   `PyMuPDF` (Fitz): For robust text extraction from PDFs.
            *   **Reasoning:** Efficient for extracting raw text and potentially layout information if needed to guide advanced cleaning or chunking.
    *   **Text Cleaning:**
        *   **Standard Practices:**
            *   Remove excessive whitespace (multiple spaces, tabs, newlines not contributing to structure).
                *   **Multiple Spaces:** Replace sequences of two or more space characters with a single space (e.g., `re.sub(r'  +', ' ', text)`).
                *   **Tabs:** Replace tab characters (`\t`) with a single space (or a fixed number of spaces, e.g., 4, if consistent indentation is sometimes meaningful, using `text.replace('\t', ' ')`).
                *   **Excessive Newlines:** Replace sequences of three or more newline characters (`\n`) with exactly two newlines to preserve paragraph breaks while removing unnecessary empty lines (e.g., `re.sub(r'\n{3,}', '\n\n', text)`).
                *   **Leading/Trailing Whitespace:** Remove all whitespace (spaces, tabs, newlines) from the beginning and end of the entire extracted text. Also, strip leading/trailing spaces and tabs from each individual line after initial newline normalization.
            *   Normalize Unicode characters (e.g., `NFKC` normalization).
            *   Handle hyphenation: Rejoin words broken across lines.
                *   **Approach:** Use regular expressions (Python's `re` module) to find patterns of a word ending with a hyphen, followed immediately by a newline, and then continuing with a letter on the next line. 
                *   **Example Regex:** `re.sub(r'([a-zA-Z]+)-\n([a-zA-Z]+)', r'\1\2', text)` would catch many common cases. This can be refined to handle punctuation or specific edge cases.
            *   `ftfy`: fixing Mojibake and other Unicode inconsistencies.
        *   **Advanced Cleaning (using NLTK - `nltk`):**
            *   **Sentence Tokenization:** NLTK's `sent_tokenize` can be used to split the extracted text into individual sentences *before* or *as part of* the chunking process. This can help create more semantically coherent chunks for RAG, as chunks are less likely to abruptly cut off mid-sentence.
                *   **Implementation:** After initial text extraction with `PyMuPDF` and basic cleaning, pass the text to `nltk.sent_tokenize()`. These sentences can then be grouped by the chunker.
        *   **Reasoning:** Clean, well-structured text is crucial. Sentence tokenization with NLTK can improve the quality of chunks for RAG by ensuring they respect sentence boundaries, leading to better context for the LLM.
    *   **Text Chunking (for RAG):**
        *   **Method:** `RecursiveCharacterTextSplitter` from `LangChain`.
        *   **Configuration:**
            *   `chunk_size`: To be determined based on typical property density and LLM context window considerations (e.g., 500-1000 characters/tokens).
            *   `chunk_overlap`: A small overlap (e.g., 50-100 characters/tokens) helps maintain context between chunks.
            *   `separators`: Prioritize common structural separators like `["\n\n", "\n", ". ", " ", ""]`. If NLTK sentence tokenization is used prior, the input to the splitter might already be a list of sentences, which can then be grouped by the splitter.
        *   **Reasoning for Recursive Character Text Splitting for RAG:** It's a versatile method that tries to split on larger semantic units (like paragraphs marked by `\n\n`) first, then smaller ones. When combined with pre-processing like sentence tokenization or by carefully choosing separators, it can create chunks that are semantically relevant, which is vital for effective RAG retrieval. The goal is for each chunk to be a self-contained piece of information that the retriever can effectively match against queries related to property groups.
    *   **Error Handling:** Log errors for PDFs that cannot be parsed.

### 3.2. AI Data Extraction Agent (AI Workflow)

*   **Description:** This component iteratively extracts over 300 predefined financial data points, organized into logical groups, using an LLM with a RAG strategy. It includes validation and refinement for extracted data.
*   **Input:**
    *   A list of cleaned text chunks from the PDF (from Section 3.1).
    *   A predefined, hardcoded structure defining logical groups of properties to be extracted. Each group contains a subset of the total 300+ properties.
*   **Output:** A single aggregated JSON object containing all extracted key-value pairs from all groups. Missing values should be explicitly indicated (e.g., `null` or "Not Found").
*   **Technical Implementation:**
    *   **Language:** Python
    *   **AI Model:** Azure OpenAI Service (e.g., GPT-4o, GPT-4 Turbo).
    *   **Overall Workflow:**
        1.  **Embedding Chunks (Once per PDF):**
            *   Use an embedding model (e.g., Azure OpenAI's `text-embedding-ada-002` or newer) to create vector embeddings for all text chunks of the current PDF. Store these in memory (e.g., a simple list or a lightweight vector store like FAISS if performance becomes an issue for many chunks, though less likely for a single PDF).
        2.  **Iterate Through Property Groups:** For each predefined logical group of properties:
            a.  **RAG - Retrieval Step for the Current Group:**
                i  **Formulate Query:** Create a descriptive query representing the information needed for the current property group (e.g., "Extract issuer details and identifiers," "What are the terms and conditions related to the underlying assets?"). This query should be designed to retrieve chunks containing information for the *entire group*.
                ii. **Embed Query:** Convert this group-specific query into an embedding using the same embedding model.
                iii. **Similarity Search:** Perform a similarity search (e.g., cosine similarity) between the group query embedding and all text chunk embeddings. Stick with the LangChain + FAISS approach. LangChain's FAISS wrapper (FAISS.similarity_search()) abstracts the complexity of the similarity search for you and is well-suited for the in-memory processing of chunks from a single PDF
                iv. **Retrieve Top-K Chunks:** Select the top K most relevant text chunks (K is tunable, e.g., 3-5 chunks). These chunks form the context for the LLM for this group.
            b.  **LLM - Extraction Step for the Current Group:**
                i.  **Construct Prompt:** Create a prompt for the Azure OpenAI LLM that includes:
                    *   The retrieved relevant text chunks (the RAG context).
                    *   Clear instructions to extract *only the specific properties listed in the current logical group*.
                    *   A request for the output to be a JSON object containing these properties.
                    *   Examples (few-shot) if helpful for consistency.
                ii. **LLM Call:** Send the prompt to the LLM.
            c.  **Validation and Refinement Loop for the Current Group:**
                i.  **Parse Output:** Attempt to parse the LLM's response as JSON. Handle potential parsing errors.
                ii. **Validate Fields:** For each successfully extracted property in the current group's JSON, apply predefined validation rules (see Section 5).
                iii. **Refinement (Max 2 Retries per Failed Property, Skip Empty):** If a non-empty property fails validation:
                    *   Construct a new, targeted prompt. Include:
                        *   The whole RAG context (retrieved chunks).
                        *   Clear feedback on why it failed (e.g., "The 'Issue Date' {value} is not a valid YYYY-MM-DD date.").
                        *   Instruct the LLM to correct *only that specific property*.
                    *   Re-invoke the LLM. Re-validate. Repeat up to 2 times.
                    *   If a property is consistently returned as empty/null/"Not Found" by the LLM, do not attempt to refine it; assume it's not present or not found in the provided context.
            d.  **Store Group Result:** Store the validated (or best-effort refined) JSON for the current group.
        3.  **Aggregate Results:** After iterating through all property groups, merge the individual JSON outputs from each group into a single, comprehensive JSON object.
    *   **Prompt Engineering:** Critical for each group. Prompts must be highly specific about the properties to extract for *that group* and the desired JSON structure for *that subgroup*.
    
### 3.3. Data Aggregation & Excel Output (Script)

*   **Description:** This component takes the final aggregated JSON data from the AI Extraction Agent and writes it into an Excel file.
*   **Input:** The single, aggregated JSON object containing all extracted properties.
*   **Output:** An Excel file (.xlsx).
*   **Technical Implementation:**
    *   **Language:** Python
    *   **Library:** `openpyxl`
    *   **Excel Structure:**
        *   A single row representing the processed PDF.
        *   Columns will correspond to all 300+ extracted data fields.
        *   *(Placeholder: Specific column names, order, and data types will be filled in by stakeholders later).*
    *   **Process:**
        1.  Flatten the aggregated JSON if it has nested structures from groups, ensuring each property maps to a column.
        2.  Create a new Excel workbook.
        3.  Write a header row with all property names.
        4.  Write the extracted data into the appropriate cells in a new row.
    *   **Error Handling:** Log any errors during Excel file generation.
    *   **Reasoning:** `openpyxl` is suitable for direct Excel generation.

### 3.4. Orchestration (Master Script)

*   **Description:** Manages the end-to-end workflow for processing a single PDF file.
*   **Technical Implementation:**
    *   **Method:** A master Python script.
    *   **Logic:**
        1.  Accept the input PDF file path.
        2.  Call the PDF Processing & Text Preprocessing/Chunking component (Section 3.1).
        3.  Receive the list of text chunks.
        4.  Pass the chunks and the predefined property group structure to the AI Data Extraction Agent (Section 3.2).
        5.  Receive the final aggregated JSON data.
        6.  Call the Data Aggregation & Excel Output component (Section 3.3) with the JSON data.
        7.  Provide a success/failure message.
    *   **Reasoning:** A single master script is adequate for this linear, single-file workflow.

## 4. Non-Functional Requirements

*   **Accuracy:** The system must achieve high precision. The grouped, RAG-based, and iterative refinement approach is designed to maximize this.
*   **Configuration (Minimal):** Azure OpenAI Service API key, endpoint, model deployment name, and embedding model name should be configurable.
*   **Logging:** Detailed logging for all stages: PDF processing, chunking, RAG retrieval (e.g., group query, number of chunks retrieved), each LLM call (prompt snippets, response snippets, validation outcomes, refinement attempts), JSON aggregation, and Excel generation.
*   **Error Handling:** Graceful error handling and clear logging for all foreseeable issues.

## 5. Data Requirements (To Be Provided by Stakeholders)

*   **Definitive List of All 300+ Fields:** A complete, precise list.
*   **Property Grouping Definition:** A clear definition of how the 300+ properties are logically grouped. For each group, list the specific properties it contains. This structure will be hardcoded.
*   **Sample PDFs:** Representative set (5-10) for development and testing.
*   **Excel Output Structure (Placeholder):** Sheet name, column headers (all 300+ fields), order, data types. *(Stakeholder to provide).*
*   **Gold Standard Data (Highly Recommended):** For each sample PDF, manually extracted "gold standard" aggregated JSON data.
*   **Validation Rules:** For each individual property: data type, regex patterns, allowed ranges/values, checksums, etc.

## 6. Assumptions

*   System processes one PDF at a time.
*   All PDFs have selectable text.
*   Azure OpenAI Service access is available.
*   The list of fields and their grouping is fixed and hardcoded.

---