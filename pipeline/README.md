# MSFT Next Day Log Return Prediction via Agentic Workflow Pipeline

This project implements a pipeline to predict the next-day log return for Microsoft (MSFT) stock. It utilizes the AGNO framework to orchestrate a series of agents, each responsible for a specific part of the machine learning workflow: Exploratory Data Analysis (EDA), Feature Engineering, Modeling, and Evaluation.

## Project Structure

```
.
├── data/                     # Directory for input data (e.g., train_clean.csv) and generated features
├── pipeline.py               # Main script to run the agentic workflow
├── EDA.py                    # Generated by EDA_Agent for exploratory data analysis
├── FEATURE.py                # Generated by Feature_Engineering_Agent for feature creation
├── MODEL.py                  # Generated by Modeling_Agent for model training
├── EVAL.py                   # Generated by Evaluation_Agent for model evaluation
├── best_model.joblib         # Saved trained model
├── MSFT Score.txt            # Output file with the model's RMSE score
├── requirements.txt          # Python dependencies
└── README.md                 # This file
```

## Prerequisites

*   Python 3.x
*   Access to Azure OpenAI Service (for the agents)

## Setup

1.  **Clone the repository (if applicable):**
    ```bash
    git clone <repository_url>
    cd <project_directory>
    ```

2.  **Create and activate a virtual environment (recommended):**
    ```bash
    python -m venv .venv
    # On Windows
    .venv\Scripts\activate
    # On macOS/Linux
    source .venv/bin/activate
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    The `requirements.txt` file should include:
    ```
    agno
    pandas
    numpy
    scikit-learn
    statsmodels
    joblib
    # Add any other specific versions if necessary, e.g.:
    # openai
    # python-dotenv
    ```

4.  **Configure Azure OpenAI Environment Variables:**
    The `pipeline.py` script (and the AGNO framework underpinning it) will require Azure OpenAI credentials to function. Set the following environment variables in your system or in a `.env` file (if using `python-dotenv` and integrating it into `pipeline.py` for model loading - though the current `pipeline.py` uses a mock model):

    *   `AZURE_OPENAI_API_KEY`: Your Azure OpenAI API key.
    *   `AZURE_OPENAI_ENDPOINT`: Your Azure OpenAI endpoint URL.
    *   `AZURE_OPENAI_DEPLOYMENT_NAME`: The name of your Azure OpenAI model deployment (e.g., gpt-4, gpt-35-turbo).
    *   `AZURE_OPENAI_API_VERSION`: The API version for Azure OpenAI (e.g., "2023-07-01-preview").

    If you are using a `.env` file, make sure it's in the root directory of the project and looks like this:
    ```
    AZURE_OPENAI_API_KEY="your_api_key"
    AZURE_OPENAI_ENDPOINT="your_endpoint_url"
    AZURE_OPENAI_DEPLOYMENT_NAME="your_deployment_name"
    AZURE_OPENAI_API_VERSION="your_api_version"
    ```
    *(Note: The current `pipeline.py` uses a mock object for the Azure model. To use a real Azure OpenAI model, you would need to modify the `run_pipeline_main_logic` function in `pipeline.py` to instantiate and configure `agno.models.azure.AzureOpenAIModel` or a similar client, and ensure these environment variables are loaded and used.)*

## Running the Pipeline

1.  **Prepare Data:**
    Ensure your input data files (e.g., `train_clean.csv`, `val_clean.csv`, `test_clean.csv`) are present in the `data/` directory. The pipeline expects these files to be in CSV format.

2.  **Execute the pipeline:**
    Run the `pipeline.py` script from the root directory of the project:
    ```bash
    python pipeline.py
    ```

## Workflow Overview

The `pipeline.py` script orchestrates four main agents:

1.  **`EDA_Agent`**:
    *   Generates and executes `EDA.py`.
    *   Performs exploratory data analysis on `train_clean.csv`.
    *   Outputs synthesized insights that are passed to the next agent.
    *   Saves `EDA.py`.

2.  **`Feature_Engineering_Agent`**:
    *   Receives insights from `EDA_Agent`.
    *   Generates and executes `FEATURE.py`.
    *   Loads `train_clean.csv`, `val_clean.csv`, `test_clean.csv`.
    *   Creates new features based on EDA insights and predefined strategies.
    *   Handles missing values and applies feature scaling.
    *   Saves `train_featured.csv`, `val_featured.csv`, `test_featured.csv` to the `data/` directory.
    *   Saves `FEATURE.py`.

3.  **`Modeling_Agent`**:
    *   Generates and executes `MODEL.py`.
    *   Loads `train_featured.csv` and `val_featured.csv`.
    *   Trains one or more regression models (e.g., XGBoost, LightGBM, MLPRegressor).
    *   Performs hyperparameter tuning (optional, based on script generation).
    *   Selects the best model based on validation RMSE.
    *   Saves the best model as `best_model.joblib` in the working directory.
    *   Saves `MODEL.py`.

4.  **`Evaluation_Agent`**:
    *   Generates and executes `EVAL.py`.
    *   Loads `test_featured.csv` and `best_model.joblib`.
    *   Makes predictions on the test set.
    *   Calculates RMSE.
    *   Saves the RMSE to `MSFT Score.txt` in the format `RMSE: <float_value>`.
    *   Saves `EVAL.py`.

## Expected Outputs

Upon successful execution, the pipeline will produce:

*   **Generated Scripts**: `EDA.py`, `FEATURE.py`, `MODEL.py`, `EVAL.py` in the root directory.
*   **Featured Data**: `train_featured.csv`, `val_featured.csv`, `test_featured.csv` in the `data/` directory.
*   **Trained Model**: `best_model.joblib` in the root directory.
*   **Evaluation Score**: `MSFT Score.txt` in the root directory, containing the RMSE.

## Troubleshooting

*   **Agent Failures**: The pipeline includes basic error handling. If an agent fails, the workflow will halt, and an error message will be printed. Check the console output for details. The agents have a built-in retry mechanism for debugging generated scripts.
*   **Dependency Issues**: Ensure all packages in `requirements.txt` are correctly installed in your active Python environment.
*   **Azure OpenAI Configuration**: If using a real Azure model, double-check that your environment variables are correctly set and that your Azure OpenAI service is active and accessible.
*   **Data Files**: Verify that the input CSV files are correctly named and located in the `data/` directory.
