# PostgreSQL Configuration
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres123
POSTGRES_DB=requirementsdb
POSTGRES_SCHEMA=public
POSTGRES_POOL_SIZE=5
POSTGRES_MAX_OVERFLOW=10
POSTGRES_POOL_TIMEOUT=30
POSTGRES_POOL_RECYCLE=1800

# Neo4j Configuration
NEO4J_URI=bolt://neo4j:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=neo4j123
NEO4J_DATABASE=neo4j
NEO4J_CONNECTION_TIMEOUT=30.0
NEO4J_MAX_CONNECTION_POOL_SIZE=50
NEO4J_MAX_TRANSACTION_RETRY_TIME=30.0

# Redis Configuration
REDIS_URL=redis://redis:6379
REDIS_PASSWORD=
REDIS_DB=0
REDIS_MAX_CONNECTIONS=10
REDIS_RETRY_ON_TIMEOUT=true
REDIS_SOCKET_CONNECT_TIMEOUT=5.0
REDIS_SOCKET_TIMEOUT=5.0

# Redis Pub/Sub Configuration
REDIS_PUBSUB_CHANNEL_PREFIX=project_progress
REDIS_PUBSUB_MAX_CONNECTIONS=5
REDIS_PUBSUB_CONNECTION_TIMEOUT=10.0

# Celery Configuration
CELERY_BROKER_URL=redis://redis:6379/0
CELERY_RESULT_BACKEND=redis://redis:6379/0

# Common Retry/Backoff Settings
RETRY_MAX_ATTEMPTS=3
RETRY_BACKOFF_BASE_SEC=2
RETRY_BACKOFF_FACTOR=2
RETRY_BACKOFF_MAX_SEC=60

# HTTP Client Configuration (shared across services)
PROJECT_MANAGEMENT_SERVICE_URL=http://project-management-service:8000
AI_REQUIREMENTS_SERVICE_URL=http://ai-requirements-service:8000
AI_TASKS_SERVICE_URL=http://ai-tasks-service:8000
GITLAB_CLIENT_SERVICE_URL=http://gitlab-client-service:8000
HTTP_CONNECTION_TIMEOUT=30.0
HTTP_READ_TIMEOUT=180.0

# Workflow Timeout (shared across AI services)
WORKFLOW_TIMEOUT_SEC=150

OAI_KEY=KEY
OAI_MODEL=gpt-4.1
# Separate embedding model name (for tiktoken) and deployment name (for Azure API)
OAI_EMBED_MODEL_NAME=text-embedding-3-small
OAI_EMBED_DEPLOYMENT_NAME=text-embedding-3-small
OAI_BASE_URL=http://openai-mock-service:8000
OAI_API_VERSION=2024-02-15-preview
OAI_EMBED_BATCH=16
OAI_EMBED_CONCURRENCY=2

# GraphRAG Concurrency/Throttling
GRAPHRAG_ASYNC_MODE=threaded
GRAPHRAG_LLM_THREAD_COUNT=16
GRAPHRAG_LLM_THREAD_STAGGER=0.2
GRAPHRAG_LLM_CONCURRENT_REQUESTS=12
GRAPHRAG_LLM_TOKENS_PER_MINUTE=100000
GRAPHRAG_LLM_REQUESTS_PER_MINUTE=60
GRAPHRAG_EMBEDDING_THREAD_COUNT=16
GRAPHRAG_EMBEDDING_THREAD_STAGGER=0.2
GRAPHRAG_EMBEDDING_CONCURRENT_REQUESTS=12
GRAPHRAG_EMBEDDING_TOKENS_PER_MINUTE=60000
GRAPHRAG_EMBEDDING_REQUESTS_PER_MINUTE=2000
GRAPHRAG_EXTRACT_MAX_GLEANINGS=3

# GraphRAG LLM Sampling Parameters (for deterministic extraction)
GRAPHRAG_LLM_TEMPERATURE=0.0
GRAPHRAG_LLM_TOP_P=1.0
GRAPHRAG_LLM_N=1

# Neo4j Vector Indexes Configuration
CHUNK_VECTOR_INDEX_NAME=graphrag_chunk_index
CHUNK_VECTOR_INDEX_LABEL=__Chunk__
COMMUNITY_VECTOR_INDEX_NAME=graphrag_comm_index
COMMUNITY_VECTOR_INDEX_LABEL=__Community__
ENTITY_VECTOR_INDEX_NAME=graphrag_entity_index
ENTITY_VECTOR_INDEX_LABEL=__Entity__
VECTOR_INDEX_PROPERTY=embedding
VECTOR_INDEX_DIMENSIONS=1536
VECTOR_INDEX_SIMILARITY=cosine

# AI Workflow / GraphRAG Client Configuration
GRAPH_RAG_BASE_URL=http://neo4j-retrieval-service:8000
RETRIEVAL_BACKOFF_BASE_SEC=0.2
RETRIEVAL_TOP_K=5

# Neo4j Retrieval Service Prompt Size Management
MAX_CHUNKS_FOR_PROMPT=20
MAX_COMMUNITIES_FOR_PROMPT=10

# AI Tasks Service Configuration
GITLAB_INGESTION_BASE_URL=http://gitlab-client-service:8000
CLARIFICATION_SCORE_TARGET=0.75
MAX_AGENT_ITERS=3
SIMILARITY_THRESHOLD=0.83
LLM_TIMEOUT_SEC=20.0
LLM_TEMPERATURE=0.2

# --- Overriding the OIDC endpoint ---
# Use host.docker.internal so both host and containers resolve the same authority
# For browser access, add "127.0.0.1 host.docker.internal" to your hosts file
# Windows: C:\Windows\System32\drivers\etc\hosts
# Linux/Mac: /etc/hosts
AZURE_AD_AUTHORITY=https://host.docker.internal:8005
AZURE_TENANT_ID=e7963c3a-3b3a-43b6-9426-89e433d07e69
AZURE_CLIENT_ID=a9e304a9-5b6c-4ef7-9b37-23a579a6d7be
AZURE_CLIENT_SECRET=mock-secret
AZURE_AD_VERIFY_SSL=false

# Mock Authentication Configuration for E2E Tests
MOCK_CLIENT_SECRET=mock-secret
MOCK_SCOPE=api://a9e304a9-5b6c-4ef7-9b37-23a579a6d7be

# UI Session and Local Auth
SESSION_SECRET_KEY=change-me-strong-secret
SESSION_COOKIE_NAME=ui_session
SESSION_MAX_AGE=1209600
SESSION_SAME_SITE=lax
ALLOW_INSECURE_SESSION=true
LOCAL_JWT_SECRET=dev-local-jwt-secret

# GitLab Instance Configuration
GITLAB_BASE_URL=https://gitlab.com
GITLAB_VERIFY_SSL=true
GITLAB_CA_CERT_PATH=/app/gitlab_client_service/gitlab.crt

# GitLab Client Service Configuration
GITLAB_CLIENT_BASE_URL=http://gitlab-client-service:8000
DEFAULT_PAGE_SIZE=100
EMBEDDINGS_PUBSUB_PREFIX=embeddings:projects:
IDEMPOTENCY_TTL_SECONDS=86400

# GitLab OAuth Configuration (handled by gitlab-client-service)
# Note: REDIRECT_URI points to ui-service (port 8007) which proxies to gitlab-client-service
GITLAB_OAUTH_CLIENT_ID=4012e709b96a4f808978153aa54c12a02e9393d53b03d283fdef1e5f27d621f5
GITLAB_OAUTH_CLIENT_SECRET=gloas-759462d63eccf7ed66f6d009f396b55459ad3fdbabb3f3b904bb070ea60fba8e
GITLAB_OAUTH_REDIRECT_URI=http://localhost:8007/auth/gitlab/callback
GITLAB_OAUTH_SCOPES=read_api api

# Azure Blob Storage Configuration (Azurite)
AZURE_STORAGE_CONNECTION_STRING=DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://azurite:10000/devstoreaccount1;
AZURE_STORAGE_CONTAINER_NAME=documents
AZURE_STORAGE_ACCOUNT_NAME=devstoreaccount1
AZURE_STORAGE_ACCOUNT_KEY=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==
AZURE_STORAGE_BLOB_ENDPOINT=http://azurite:10000/devstoreaccount1
AZURE_STORAGE_MAX_SINGLE_PUT_SIZE=67108864
AZURE_STORAGE_MAX_BLOCK_SIZE=4194304

# Tika Configuration
TIKA_SERVER_JAR=/opt/tika-server/tika-server.jar
TIKA_SERVER_ENDPOINT=http://localhost:9998
TIKA_LOG_PATH=/tmp/tika-logs
TIKA_SERVER_TIMEOUT=300
TIKA_CLIENT_TIMEOUT=120
TIKA_VERSION=3.1.0
TIKA_CLIENT_ONLY=true
TIKA_SERVER_AUTO_START=true
TIKA_SERVER_STARTUP_TIMEOUT=60

# Docling VLM Configuration
# VLM Mode Selection: local (SmolVLM) or remote (API-based)
DOCLING_VLM_MODE=local
# Remote VLM Provider: azure_openai, lm_studio, ollama, watsonx, openai_compatible
DOCLING_VLM_PROVIDER=azure_openai

# Azure OpenAI Configuration (PRIMARY remote provider)
DOCLING_AZURE_OPENAI_ENDPOINT=
DOCLING_AZURE_OPENAI_DEPLOYMENT_NAME=
DOCLING_AZURE_OPENAI_API_KEY=
DOCLING_AZURE_OPENAI_API_VERSION=2024-02-15-preview

# Generic Remote VLM Configuration (for non-Azure providers)
DOCLING_VLM_ENDPOINT=http://localhost:1234
DOCLING_VLM_MODEL=granite-docling-258m-mlx
DOCLING_VLM_API_KEY=

# watsonx.ai Configuration
WX_API_KEY=
WX_PROJECT_ID=

# Common VLM Parameters
DOCLING_VLM_PROMPT=Convert this page to docling format with detailed descriptions.
DOCLING_VLM_TIMEOUT=90
DOCLING_VLM_TEMPERATURE=0.7
DOCLING_VLM_MAX_TOKENS=4096
DOCLING_VLM_RESPONSE_FORMAT=DOCTAGS

# Local VLM Configuration (when DOCLING_VLM_MODE=local)
DOCLING_USE_OCR=true
DOCLING_OCR_LANGS=en
DOCLING_IMAGES_SCALE=2.0
DOCLING_IMAGE_EXTENSIONS=.png,.jpg,.jpeg,.tiff,.tif,.bmp,.gif,.webp,.heic,.heif,.ppm,.pbm,.pgm
DOCLING_TIMEOUT_SECONDS=180

# Backfill Description Configuration
BACKFILL_BATCH=100
BACKFILL_MAX_WORKERS=4